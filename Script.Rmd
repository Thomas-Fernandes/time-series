---
title: "Script"
output: pdf_document
date: "2024-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tseries)
library(forecast)
library(kableExtra)
library(knitr)
```


# Exercice 1

## a) $X_t = \sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}$

**1. Espérance**

$\mathbb{E}[\epsilon_t] = 0$ puisqu'il s'agit d'un bruit blanc et $\mathbb{E}[\sin(t)]$ varie en fonction de $t$. Par conséquent, l'espérance de \(X_t\) n'est pas constante, car elle dépend de \(t\) à travers le terme \(\sin(t)\).

**2. Autocovariance**

La fonction d'autocovariance est définie par $\gamma_X(h) = \text{Cov}(X_t, X_{t+h}), \quad h = 0, \pm 1, \pm 2, \ldots$. Pour \(X_t\), nous avons :

\[ \gamma_X(h) = \text{Cov}(\sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}, \sin(t+h) + \varepsilon_{t+h} - 0.2 \times \varepsilon_{t+h-1}) \]

Puisque \( \varepsilon_t \) est un bruit blanc de variance \(\sigma^2\) et est indépendant des termes précédents et suivants, la seule autocovariance non nulle se produit lorsque \( h = 0 \) ou \( h = \pm1 \). Cela donne :
\begin{align*}
\gamma_X(0)   &= \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) \\
\gamma_X(1)   &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t+1} - 0.2 \times \varepsilon_t) \\
\gamma_X(-1)  &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t-1} - 0.2 \times \varepsilon_{t-2})
\end{align*}


- \( h = 0 \) : \( \gamma_X(0) = \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) = \sigma^2 + 0.04\sigma^2 = 1.04\sigma^2 \) (car \( \sigma^2 = 1 \) dans les simulations).
- \( h = 1 \) ou \( h = -1 \) : \( \gamma_X(1) = \gamma_X(-1) = -0.2\sigma^2 \).

La dépendance de \( \gamma_X(h) \) sur \( t \) (due à la présence de \( \sin(t) \) et \( \sin(t+h) \)) indique que le processus n'est pas stationnaire au second ordre.

**3. Décomposition saisonnière**

Pour rendre ce processus stationnaire, nous allons enlever la composante de saisonnalité. Ainsi :

\(Y_t = X_t - \sin(t) = \varepsilon_t - 0.2 \times \varepsilon_{t-1}\).

Le processus peut ainsi être réécris sous la forme $X_t = (1-0.2B)\epsilon_t$, correspondant à un MA(1).

```{r}
set.seed(123)
a1 <- arima.sim(model = list(ma = 0.2), n = 200)
a2 <- arima.sim(model = list(ma = 0.2), n = 200)
a3 <- arima.sim(model = list(ma = 0.2), n = 200)

plot(a1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(a2, col="limegreen")
lines(a3, col="lightblue")
```


## b) $\, X_t = \varepsilon_t - \varepsilon_{t-1}$

**1. Espérance**

\[
E(X_t) = E(\varepsilon_t - \varepsilon_{t-1}) = E(\varepsilon_t) - E(\varepsilon_{t-1}) = 0 - 0 = 0
\]

**2. Autocovariance**

- Pour \( h = 0 \) :
\begin{align*}
\gamma_X(0) &= \text{Cov}(X_t, X_{t + 0}) \\
&= \text{Var}(X_t) \\
&= \text{Var}(\varepsilon_t) + \text{Var}(\varepsilon_{t-1}) \\
&= \sigma^2 + \sigma^2 \\
&= 2\sigma^2
\end{align*}

- Pour \( h = 1 \) :
\begin{align*}
\gamma_X(1) &= \text{Cov}(X_t, X_{t+1}) \\
&= \text{Cov}(\varepsilon_t - \varepsilon_{t-1}, \varepsilon_{t+1} - \varepsilon_t) \\
&= \text{Cov}(\varepsilon_t, \varepsilon_{t+1}) - \text{Cov}(\varepsilon_t, \varepsilon_t) - \text{Cov}(\varepsilon_{t-1}, \varepsilon_{t+1}) + \text{Cov}(\varepsilon_{t-1}, \varepsilon_t)
\end{align*}

D'après la définition du bruit blanc, $\text{Cov}(\varepsilon_t, \varepsilon_s) = 0, \quad t \neq s$, alors :
\begin{align*}
\gamma_X(1) &= 0 - \text{Var}(\varepsilon_t) - 0 + 0 \\
&= -\text{Var}(\varepsilon_t) \\
&= -\sigma^2
\end{align*}

- Pour \( h > 1 \) ou \( h < -1 \) :
Les termes sont indépendants, donc \( \gamma_X(h) = 0 \).

Le processus \( X_t = \varepsilon_t - \varepsilon_{t-1} \) est stationnaire au second ordre car son espérance est constante et sa fonction d'autocovariance ne dépend que de \( h \) et non de \( t \).


**3. Forme canonique**

Le processus, sous sa forme canonique, s'écrit comme : $X_t = (1 - B)\varepsilon_t$ correspondant à un un MA(1).

```{r}
set.seed(123)
b1 <- arima.sim(model = list(ma = -1), n = 200)
b2 <- arima.sim(model = list(ma = -1), n = 200)
b3 <- arima.sim(model = list(ma = -1), n = 200)

plot(b1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(b2, col="limegreen")
lines(b3, col="lightblue")
```

## c) $X_t = A\cos(\omega t) + B\sin(\omega t)$ Je suis vraiment pas sur de ça, j'arrive pas à trouver la forme canonique donc faut probablement le différencier et c'est probablement faux

**1. Espérance**

$E[X_t] = E[A]\cos(\omega t) + E[B]\sin(\omega t) = 0$

**2. Autocovariance**
\begin{align*}
\gamma(s, t) &= E\left[(A\cos(\omega s) + B\sin(\omega s))(A\cos(\omega t) + B\sin(\omega t))\right] \\
&= E\left[A^2\cos(\omega s)\cos(\omega t) + AB\cos(\omega s)\sin(\omega t) + AB\sin(\omega s)\cos(\omega t) + B^2\sin(\omega s)\sin(\omega t)\right]
\end{align*}


Étant donné que $A$ et $B$ sont indépendantes, on peut simplifier cette expression en utilisant \( E[A^2] \), \( E[B^2] \) et le fait que \( E[AB] = E[A]E[B] = 0 \) :

\[ \gamma(s, t) = E[A^2]\cos(\omega s)\cos(\omega t) + E[B^2]\sin(\omega s)\sin(\omega t) \]

Puisque \( A \) et \( B \) ont une variance finie, disons \( \sigma_A^2 \) et \( \sigma_B^2 \) respectivement, nous pouvons remplacer \( E[A^2] \) et \( E[B^2] \) par leurs variances :

\[ \gamma(s, t) = \sigma_A^2\cos(\omega s)\cos(\omega t) + \sigma_B^2\sin(\omega s)\sin(\omega t) \]

Nous pouvons utiliser l'identité trigonométrique pour les produits de cosinus et de sinus :

\[
\cos x \cos y = \frac{1}{2}[\cos(x-y) + \cos(x+y)]
\]
\[
\sin x \sin y = \frac{1}{2}[\cos(x-y) - \cos(x+y)]
\]

En appliquant ces identités à notre expression, nous obtenons :

\[
\gamma(s, t) = \sigma_A^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) + \cos(\omega s + \omega t)] + \sigma_B^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) - \cos(\omega s + \omega t)]
\]

En simplifiant cette expression, les termes en \( \cos(\omega s + \omega t) \) se soustraient mutuellement, laissant :

\[
\gamma(s, t) = \frac{1}{2}(\sigma_A^2 + \sigma_B^2) \cos(\omega(t - s))
\]

Le processus \( X_t = A\cos(\omega t) + B\sin(\omega t) \) est stationnaire au second ordre, car son espérance est constante (et égale à zéro) et sa fonction d'autocovariance dépend uniquement de la différence entre les temps \( s \) et \( t \), et non des temps individuels.

```{r}
n <- 200
omega <- 2 * pi / 40

set.seed(1)
A <- rnorm(3)
B <- rnorm(3)

t <- 1:n
trajectories <- matrix(0, nrow = n, ncol = 3)

for (i in 1:3) {
  trajectories[, i] <- A[i] * cos(omega * t) + B[i] * sin(omega * t)
}

# Tracer les trajectoires
plot(t, trajectories[, 1], type = 'l', col = 'red', ylim = range(trajectories),
     xlab = 'Time', ylab = 'X_t', main = 'Three Trajectories of X_t')
lines(t, trajectories[, 2], col = 'green')
lines(t, trajectories[, 3], col = 'blue')
```



## d) $X_t = 2X_{t-1} - X_{t-2} + \varepsilon_t$

**1. Espérance :**
\begin{align*}
\mathbb{E}[X_t] & = 2\mathbb{E}[X_t] - \mathbb{E}[X_t] + \mathbb{E}[\varepsilon_t] \\
\mathbb{E}[X_t] & = \mathbb{E}[X_t] + 0
\end{align*}

Le processus est égale à lui-même et donc constant dans le temps.

**2. Fonction d'autocovariance**


**3. Forme canonique**

Le modèle peut se réécrire sous sa forme canonique comme : $(1 - 2B + B^2) X_t = \varepsilon_t$, correspondant à un AR(2).

On cherche les racines du polynome caractéristique $1-2r+r^2=0$ :

\[ r = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \]

Dans notre cas, \( a = 1 \), \( b = -2 \) et \( c = 1 \). Les racines seront donc :

\[ r = \frac{-(-2) \pm \sqrt{(-2)^2 - 4 \cdot 1 \cdot 1}}{2 \cdot 1} \]
\[ r = \frac{2 \pm \sqrt{4 - 4}}{2} \]
\[ r = \frac{2 \pm \sqrt{0}}{2} \]
\[ r = \frac{2}{2} \]
\[ r = 1 \]

Le processus n'est donc pas stationnaire puisque les racines sont sur le cercle unité.



## e) $\quad X_t - 4X_{t-1} = \varepsilon_t - 0.25\varepsilon_{t-1}$

**1. Espérance**

L'espérance \( E(X_t) \) d'un processus stationnaire doit être constante. Calculons-la :

\[
E(X_t - 4X_{t-1}) = E(\varepsilon_t - 0.25\varepsilon_{t-1})
\]

Étant donné que \( \varepsilon_t \) est un bruit blanc, \( E(\varepsilon_t) = 0 \) pour tout \( t \). Par conséquent :

\[
E(X_t - 4X_{t-1}) = 0 - 0.25 \times 0 = 0
\]

Cela implique que \( E(X_t) = 4E(X_{t-1}) \). Pour qu'un processus soit stationnaire, il faut que \( E(X_t) \) soit le même pour tout \( t \). Ici, nous n'avons pas assez d'informations pour déterminer si \( E(X_t) \) est constant sans connaître la valeur de \( E(X_{t-1}) \). 


**2. Fonction d'autocovariance**

Comme pour l'espérance, nous n'avons pas assez d'informations pour déterminer si \( E(X_t) \) est constant sans connaître la valeur de \( E(X_{t-1}) \).

**3. Forme canonique**

Nous pouvons réécrire le processus comme $(1 - 4B)X_t = (1 - 0.25B)\varepsilon_t$.

Solution de l'équation caractéristique pour la partie AR :

$1-4r=0$
$r=0.25$

Solution de l'équation caractéristique pour la partie MA :

$1-0.25r=0$
$r=4$

Notre processus est donc un ARMA(0.25, 4)


# Exercice 2

## a) Représentation graphique de la série p1

```{r}
source("prix.R")

plot(p1, type = "l", main = "Série Temporelle des Prix", xlab = "Temps", ylab = "Prix")
```
D'après le graphique, on peut émettre l'hypothèse que la série est stationnaire en moyenne et en variance puisque la moyenne est constante et la variance homoscédastique. De plus, la covariance entre 2 points semble constante, l'écart entre chaque fluctuation est le même et laisse penser à une saisonalité.


## b) Fonctions d'autocorrélation acf et pacf

```{r}
acf(p1)
pacf(p1)
spectrum(p1)
p1_decomp <- decompose(p1)
plot(p1_decomp)
```

On remarque que la saisonalité est très marquée et régulière, tandis que la tendance est plus compliquée à discerner, même s'il semble y avoir une tendance à la baisse.

## c) $X_t = m + A \sin\left(\frac{2\pi t}{T}\right) + B \cos\left(\frac{2\pi t}{T}\right) + Y_t$

   - \( X_t \) représente la valeur de la série temporelle au temps \( t \).
   - \( m \) est la moyenne de la série.
   - \( A \) et \( B \) sont les amplitudes des composantes sinusoïdales et cosinusoïdales, respectivement.
   - \( T \) est la période de la composante saisonnière. Pour des données hebdomadaires sur plusieurs années, \( T \) pourrait être 52, représentant le nombre de semaines par an.
   - \( Y_t \) est le terme d'erreur ou la composante non-périodique.

On choisit donc un T de 52.

```{r}
T <- 52
t <- 1:length(p1)

sin_term <- sin(2 * pi * t / T)
cos_term <- cos(2 * pi * t / T)

reg_model <- lm(p1 ~ sin_term + cos_term)

m <- coef(reg_model)[1]
A <- coef(reg_model)[2]
B <- coef(reg_model)[3]

#centrer les résidus
Zt <- reg_model$residuals - mean(reg_model$residuals)

#Test de Dickey-fuller (test de stationnarité)
adf_test <- adf.test(Zt)

summary(reg_model)
adf_test
```

On remarque que tous les coefficients sont significatifs. La composante saisonnière sinusoïdale est donc une caractéristique importante de la série p1. L'écart type résiduel est faible (0.5328) et le coefficient de détermination ajusté est plutôt élevé (0.862), le modèle semble donc expliqué une grande partie de la variance des données.

Le p.value du test de Dickey-Fuller étant inférieur au seuil de 5%, on rejette l'hypothèse nulle d'une racine unitaire. Les résidus $Z_t$ du modèle sont stationnaires.

## d) Choix du modèle

```{r}
acf(Zt)
pacf(Zt)
```

$Z_t$ n'est pas un bruit blanc puisqu'il y a des autocorrelations significatives sur l'acf comme sur le pacf, suggérant que notre modèle contiendra une partie AR et une partie MA. Sur la partie AR, le dernier retard est atteint en lag = 4 et pour la partie MA, le dernier retard est atteint en lag = 6. Nous allons ainsi tester par le moyen d'une boucle tous les modèles en faisant varier la partie AR de 0 à 4 et la partie MA de 0 à 6. Pour chaque modèle, on vérifiera les t.values associées à chaque coefficients $coef/s.e.$ en considérant que si sa valeur est inférieure à 1.96, considère la t.value comme non significative.

```{r}
library(forecast)

# Generate all possible combinations of AR and MA orders
orders <- expand.grid(ar = 0:4, ma = 0:6)[-1, ] # exclude the case where both ar and ma are zero

# Fit an ARIMA model for each combination
models <- list()
for (i in seq(nrow(orders))) {
  models[[i]] <- Arima(Zt, order = c(orders[i, "ar"], 0, orders[i, "ma"]))
}

# Compute the AIC and count non-significant t values for each model
results <- data.frame(AR = rep(NA, nrow(orders)),
                      MA = rep(NA, nrow(orders)),
                      AIC = rep(NA, nrow(orders)),
                      stringsAsFactors = FALSE)
for (i in seq(nrow(orders))) {
  model <- models[[i]]
  coefs <- model$coef
  se_coefs <- sqrt(diag(vcov(model)))
  t.values <- abs(coefs / se_coefs)
  compteur_t.values <- sum(t.values < 1.96) - 1

  results[i, "AR"] <- orders[i, "ar"]
  results[i, "MA"] <- orders[i, "ma"]
  results[i, "AIC"] <- AIC(model)
  results[i, "t.value non significatives"] <- compteur_t.values
}

kable(results, booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```

Il y a deux modèles qui se distinguent clairement des autres : ARMA(1,3) et l'ARMA(4,1). Ils ont toutes leurs t.values significatives et ont un AIC plus faible que les autres. On gardera l'ARMA(1,3) car son AIC est un petit peu plus faible que pour l'ARMA(4,1), bien qu'on puisse considérer les deux modèles comme équivalents.

## e) Prévision pour le mois de janvier

Pour la fonction forecast, on choisit un h = 4, correspondant aux 4 premières semaines de janvier.

```{r}
arma_xt <- Arima(Zt, order=c(1,0,3))
Zt_forecast <- forecast(arma_xt, h=4)
```

On doit ensuite reconstruire les prédictions pour $X_t$ en ajoutant la trend et la saisonalité.

```{r}
t_future <- length(p1) + 1:length(Zt_forecast$mean)
sin_term_future <- sin(2 * pi * t_future / 52)
cos_term_future <- cos(2 * pi * t_future / 52)
Xt_future <- m + A * sin_term_future + B * cos_term_future + Zt_forecast$mean

# Afficher les prédictions pour Xt
Xt_future

```

## f) SARIMA

```{r}
sarima_Xt <- auto.arima(p1)
summary(sarima_Xt)
```

La fonction `auto.arima()` choisi le modèle $SARIMA_{52}[(0,0,3)(2,1,0)]$. Ainsi, il choisi 3 composantes MA pour la partie non saisonnière et 2 composantes AR ainsi qu'une différenciation pour la partie saisonnière.

```{r}
january_forecast <- forecast(sarima_Xt, h=4)
january_forecast
```

## g) Application de l'opérateur de différenciation $(1-B^52)$

```{r}
p1_diff <- diff(p1, lag = 52)
arma_model_diff <- auto.arima(p1_diff, seasonal = FALSE)
summary(arma_model_diff)
january_forecast_diff <- forecast(arma_model_diff, h=4)
kbl(january_forecast_diff, booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```

## h) Modèle le plus pertinent

## i) 

$(1 - B^2)(1 - 2B + B^2)(1 - 2B)X_t = \varepsilon_t - \varepsilon_{t-1}$\\
$(1 - B^2)(1 - 2B + B^2)(1 - 2B) = (1 - B)(1 + B)(1 - B)(1 - B) = (1 - B)^3(1 + B)$\\
$(1 - B)^3(1 + B)X_t = \varepsilon_t - \varepsilon_{t-1}$\\

