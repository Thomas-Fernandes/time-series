---
title: "Projet de Séries temporelles, M1 IES-D3S"
output: pdf_document
date: "2024-01-16"
author: "Thomas Fernandes, Vanessa Kenniche, Yassine Ouerghi"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tseries)
library(forecast)
library(kableExtra)
library(knitr)
library(sarima)
library(TSA)
```

# Exercice 1

## a) $X_t = \sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}$

**1. Espérance**
\begin{align*}
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)] + \mathbb{E}[\varepsilon_t] - 0.2\mathbb{E}[\varepsilon_{t-1}] \\
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)] + 0 - 0.2 \times 0 \\
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)]
\end{align*}

Par conséquent, l'espérance de $X_t$ n'est pas constante car elle dépend de t à travers le terme $\sin(t)$.

**2. Autocovariance**

\[ \gamma_X(h) = \text{Cov}(\sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}, \sin(t+h) + \varepsilon_{t+h} - 0.2 \times \varepsilon_{t+h-1}) \]

La covariance des sinus s'annule donc :
\begin{align*}
\gamma_X(0)   &= \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) \\
\gamma_X(1)   &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t+1} - 0.2 \times \varepsilon_t) \\
\gamma_X(-1)  &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t-1} - 0.2 \times \varepsilon_{t-2})
\end{align*}


- \( h = 0 \) : \( \gamma_X(0) = \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) = \sigma^2 + 0.04\sigma^2 = 1.04 \)
- \( h = 1 \) ou \( h = -1 \) : \( \gamma_X(1) = \gamma_X(-1) = -0.2\sigma^2 \).

Le processus n'est ainsi pas stationnaire puisque son espérance n'est pas constante.

**3. Décomposition saisonnière**

Pour rendre ce processus stationnaire, nous allons enlever la composante de saisonnalité. Ainsi :

\(Y_t = X_t - \sin(t) = \varepsilon_t - 0.2 \times \varepsilon_{t-1}\).

Le processus peut ainsi être réécris sous la forme $X_t = (1-0.2B)\epsilon_t$, correspondant à un MA(1).

```{r}
set.seed(123)
a1 <- arima.sim(model = list(ma = 0.2), n = 200)
a2 <- arima.sim(model = list(ma = 0.2), n = 200)
a3 <- arima.sim(model = list(ma = 0.2), n = 200)

plot(a1, type="l", col="black", main="Trajectoires MA(1)",
     xlab = "Temps", ylab = "Valeurs")
lines(a2, col="limegreen")
lines(a3, col="lightblue")
```

## b) $\, X_t = \varepsilon_t - \varepsilon_{t-1}$

**1. Espérance**
\[
E(X_t) = E(\varepsilon_t - \varepsilon_{t-1}) = E(\varepsilon_t) - E(\varepsilon_{t-1}) = 0 - 0 = 0
\]

**2. Autocovariance**

- Pour \( h = 0 \) :
\begin{align*}
\gamma_X(0) &= \text{Cov}(X_t, X_{t + 0}) \\
&= \text{Var}(X_t) \\
&= \text{Var}(\varepsilon_t) + \text{Var}(\varepsilon_{t-1}) \\
&= \sigma^2 + \sigma^2 \\
&= 2
\end{align*}
- Pour \( h = 1 \) :
\begin{align*}
\gamma_X(1) &= \text{Cov}(X_t, X_{t+1}) \\
&= \text{Cov}(\varepsilon_t - \varepsilon_{t-1}, \varepsilon_{t+1} - \varepsilon_t) \\
&= \text{Cov}(\varepsilon_t, \varepsilon_{t+1}) - \text{Cov}(\varepsilon_t, \varepsilon_t) - \text{Cov}(\varepsilon_{t-1}, \varepsilon_{t+1}) + \text{Cov}(\varepsilon_{t-1}, \varepsilon_t)
\end{align*}

D'après la définition du bruit blanc, $\text{Cov}(\varepsilon_t, \varepsilon_s) = 0, \quad t \neq s$, alors :
\begin{align*}
\gamma_X(1) &= 0 - \text{Var}(\varepsilon_t) - 0 + 0 \\
&= -\text{Var}(\varepsilon_t) \\
&= -1
\end{align*}

Le processus \( X_t = \varepsilon_t - \varepsilon_{t-1} \) est stationnaire au second ordre car son espérance est constante et sa fonction d'autocovariance ne dépend que de \( h \) et non de \( t \).

**3. Forme canonique**

Le processus, sous sa forme canonique, s'écrit comme : $X_t = (1 - B)\varepsilon_t$ correspondant à un un MA(1).

```{r}
set.seed(123)
b1 <- arima.sim(model = list(ma = 1), n = 200)
b2 <- arima.sim(model = list(ma = 1), n = 200)
b3 <- arima.sim(model = list(ma = 1), n = 200)

plot(b1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(b2, col="limegreen")
lines(b3, col="lightblue")
```

## c) $X_t = A\cos(\omega t) + B\sin(\omega t)$

**1. Espérance**
\begin{align*}
E[X_t] &= E[A]\cos(\omega t) + E[B]\sin(\omega t) \\
E[X_t] &= E[A] \times E[\cos(\omega t)] + E[B] \times E[\sin(\omega t)] = 0
\end{align*}

**2. Autocovariance**
\begin{align*}
\gamma(s, t) &= E\left[(A\cos(\omega s) + B\sin(\omega s))(A\cos(\omega t) + B\sin(\omega t))\right] \\
&= E\left[A^2\cos(\omega s)\cos(\omega t) + AB\cos(\omega s)\sin(\omega t) + AB\sin(\omega s)\cos(\omega t) + B^2\sin(\omega s)\sin(\omega t)\right]
\end{align*}

Étant donné que $A$ et $B$ sont indépendants, on peut simplifier cette expression en utilisant \( E[A^2] \), \( E[B^2] \) et le fait que \( E[AB] = E[A]E[B] = 0 \) :

\[ \gamma(s, t) = E[A^2]\cos(\omega s)\cos(\omega t) + E[B^2]\sin(\omega s)\sin(\omega t) \]

Puisque \( A \) et \( B \) ont une variance finie on peut les remplacer par leurs variances :
\begin{align*}
\gamma(s, t) &= \sigma_A^2\cos(\omega s)\cos(\omega t) + \sigma_B^2\sin(\omega s)\sin(\omega t) \\
&= \sigma_A^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) + \cos(\omega s + \omega t)] + \sigma_B^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) - \cos(\omega s + \omega t)] \\
&= \frac{1}{2}(\sigma_A^2 + \sigma_B^2) \cos(\omega(t - s))
\end{align*}

Le processus \( X_t = A\cos(\omega t) + B\sin(\omega t) \) est stationnaire au second ordre, car son espérance est constante et sa fonction d'autocovariance dépend uniquement de la différence entre les temps \( s \) et \( t \).

```{r}
set.seed(123)
n <- 200
omega <- pi/40
A <- rnorm(n)
B <- rnorm(n)

t <- 1:n
trajectoires <- matrix(0, nrow = n, ncol = 3)

for (i in 1:3) {
  trajectoires[, i] <- A[i] * cos(omega * t) + B[i] * sin(omega * t)
}

plot(t, trajectoires[, 1], type = 'l', col = 'black', ylim = range(trajectoires),
     xlab = 'Temps', ylab = 'X_t', main = 'Trajectoires pour omega = pi/40')
lines(t, trajectoires[, 2], col = 'limegreen')
lines(t, trajectoires[, 3], col = 'lightblue')
```

Ce qui est intéressant avec ce processus, c'est qu'il est déterministe. En connaissant le $A$, $B$ et $\omega$ de départ, on peut déterminer toute la trajectoire de $X_t$. Cependant, notre processus est défini pour $\omega$ fixé dans $[0, 2\pi]$ et simulé sur n = 200. Or, si on prend un $\omega$ proche de $0$ mais $\ne0$, la trajectoire de $X_t$ s'apparentera à celle d'un processus non-stationnaire.

```{r}
set.seed(123)
n <- 200
omega <- pi/2000
A <- rnorm(n)
B <- rnorm(n)

t <- 1:n
trajectoires <- matrix(0, nrow = n, ncol = 3)

for (i in 1:3) {
  trajectoires[, i] <- A[i] * cos(omega * t) + B[i] * sin(omega * t)
}

plot(t, trajectoires[, 1], type = 'l', col = 'black', ylim = range(trajectoires),
     xlab = 'Temps', ylab = 'X_t', main = 'Trajectoires pour omega = pi/2000')
lines(t, trajectoires[, 2], col = 'limegreen')
lines(t, trajectoires[, 3], col = 'lightblue')
```

## d) $X_t = 2X_{t-1} - X_{t-2} + \varepsilon_t$

**1. Espérance :**
\begin{align*}
\mathbb{E}[X_t] & = 2\mathbb{E}[X_t] - \mathbb{E}[X_t] + \mathbb{E}[\varepsilon_t] \\
\mathbb{E}[X_t] & = \mathbb{E}[X_t] + 0
\end{align*}

Le processus est égale à lui-même et donc constant dans le temps.

**3. Forme canonique**

Le modèle peut se réécrire sous sa forme canonique comme : $(1 - 2B + B^2) X_t = \varepsilon_t$, correspondant à un AR(2).

1 est racine évidente du polynôme caractéristique $(1 - 2B + B^2)$.

Le processus n'est donc pas stationnaire puisqu'il a une racine sur le cercle unité.

**4. Différenciation**

Pour le rendre stationnaire, on va différencier le processus à l'ordre 2.

\begin{align*}
\Delta X_t &= (2X_{t-1} - X_{t-2} + \varepsilon_t) - X_{t-1} \\
\Delta X_t &= X_{t-1} - X_{t-2} + \varepsilon_t \\
\\
\Delta^2 X_t &= \Delta X_t - \Delta X_{t-1} \\
\Delta^2 X_t &= (X_{t-1} - X_{t-2} + \varepsilon_t) - (X_{t-2} - X_{t-3} + \varepsilon_{t-1}) \\
\Delta^2 X_t &= X_{t-1} - 2X_{t-2} + X_{t-3} + \varepsilon_t - \varepsilon_{t-1}
\end{align*}


Le processus correspond donc à un ARIMA(2,2,0).

```{r}
processus_d <- function(n) {
  epsilon <- rnorm(n)
  X <- rep(0, n)
  delta2_X <- rep(0, n)

  X[1] <- rnorm(1)
  X[2] <- rnorm(1)
  X[3] <- rnorm(1)

  for (t in 4:n) {
    X[t] <- 2*X[t-1] - X[t-2] + epsilon[t]
    delta2_X[t] <- X[t-1] - 2*X[t-2] + X[t-3] + epsilon[t] - epsilon[t-1]
  }

  return(delta2_X)
}

n = 200
d1 <- processus_d(n)
d2 <- processus_d(n)
d3 <- processus_d(n)

plot(d1, type="l", col="black", main="Trajectoires", xlab="Temps", ylab="Valeurs")
lines(d2, col="limegreen")
lines(d3, col="lightblue")
```

## e) $\quad X_t - 4X_{t-1} = \varepsilon_t - 0.25\varepsilon_{t-1}$

Nous pouvons réécrire le processus comme $(1 - 4B)X_t = (1 - 0.25B)\varepsilon_t$.

Solution de l'équation caractéristique pour la partie AR :
\begin{align*}
1 - 4r &= 0 \\
r &= 0.25
\end{align*}
Solution de l'équation caractéristique pour la partie MA :
\begin{align*}
1 - 0.25r &= 0 \\
r &= 4
\end{align*}
La racine est l’extérieur du cercle unité, on inverse et on trouve $r=0.25$. Notre processus est donc un ARMA(0.25, 0.25).

```{r}
set.seed(123)
e1 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)
e2 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)
e3 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)

plot(e1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(e2, col="limegreen")
lines(e3, col="lightblue")
```

\newpage

# Exercice 2

## a) Représentation graphique de la série p1

```{r}
source("prix.R")
plot(p1, type = "l", main = "Série Temporelle des Prix", xlab = "Temps", ylab = "Prix")
```
D'après le graphique, on peut émettre l'hypothèse que la série est stationnaire en moyenne et en variance puisque la moyenne est constante et la variance homoscédastique. De plus, la covariance entre 2 points semble constante, l'écart entre chaque fluctuation est le même et laisse penser à une saisonnalité. On peut cependant constater une légère tendance à la baisse à partir de 2019.

## b) Fonctions d'autocorrélation acf et pacf

```{r}
acf(p1)
pacf(p1)
spectrum(p1)
p1_decomp <- decompose(p1)
plot(p1_decomp)
```
 - ACF : On observe une autocorrelation qui s'apparente à un sinusoïde, cela indique souvent la présence d'une racine unitaire ou d'une tendance dans la série temporelle, ce qui suggère que la série n'est pas stationnaire. \
 - PACF : Comme le deuxième lag ne se situe pas dans la borne de confiance, on peut dire que notre processus n'est pas un bruit blanc. \
 - Périodogramme : Il nous indique par la présence des pics que la série temporelle a une variance significative, pouvant correspondre à des cycles ou saisonnalités.

- Decompose :
  - Composante observée (en haut) : C'est la série temporelle originale qui combine les effets de la tendance, de la saisonnalité et des composantes aléatoires.
  - Composante de tendance (deuxième graphique) : Cette ligne lisse montre comment la série évolue sur le long terme, sans tenir compte des fluctuations saisonnières. On peut voir que la tendance est relativement stable mais présente une légère baisse et hausse périodique.
  - Composante saisonnière (troisième graphique) : Cela montre un motif clair et répétitif qui semble se reproduire chaque année. La régularité et l'amplitude des pics et des creux indiquent que la série temporelle est influencée par des facteurs saisonniers, confirmant l'hypothèse faite en a).
  - Composante aléatoire (en bas) : Ce qui reste après avoir retiré la tendance et la saisonnalité de la série observée. Dans une série temporelle bien modélisée, cette composante devrait ressembler à un bruit blanc.

## c) $X_t = m + A \sin\left(\frac{2\pi t}{T}\right) + B \cos\left(\frac{2\pi t}{T}\right) + Y_t$

On choisit un T de 52 correspondant aux nombre de semaines.

```{r, warning=FALSE}
T <- 52
t <- 1:length(p1)

sin_terme <- sin(2 * pi * t / T)
cos_terme <- cos(2 * pi * t / T)

reg_modele <- lm(p1 ~ sin_terme + cos_terme)

m <- coef(reg_modele)[1]
A <- coef(reg_modele)[2]
B <- coef(reg_modele)[3]

Zt <- reg_modele$residuals - mean(reg_modele$residuals)

adf_test <- adf.test(Zt)
summary(reg_modele)
adf_test
```

On remarque que tous les coefficients sont significatifs. La composante saisonnière sinusoïdale est donc une caractéristique importante de la série p1. L'écart type résiduel est faible (0.5328) et le coefficient de détermination ajusté est plutôt élevé (0.862), le modèle semble donc expliqué une grande partie de la variance des données.

Le p.value du test de Dickey-Fuller étant inférieur au seuil de 5%, on rejette l'hypothèse nulle d'une racine unitaire. Les résidus $Z_t$ du modèle sont stationnaires.

## d) Choix du modèle

```{r}
acf(Zt)
pacf(Zt)
Box.test(Zt, type = "Ljung-Box")
```

$Z_t$ n'est pas un bruit blanc puisqu'il y a des autocorrelations significatives sur l'acf comme sur le pacf, suggérant que notre modèle contiendra une partie AR et une partie MA. Sur la partie AR, le dernier retard est atteint en lag = 4 et pour la partie MA, le dernier retard est atteint en lag = 6. En complément, nous effectuons un test de Ljung-Box afin de confirmer cette hypothèse. Effectivement, sa p.value supérieure à 0.05 confirme l'hypothèse que ce n'est pas un bruit blanc.

Nous allons ainsi tester par le moyen d'une boucle tous les modèles en faisant varier la partie AR de 0 à 4 et la partie MA de 0 à 6. Pour chaque modèle, on vérifiera les t.values associées à chaque coefficients $coef/s.e.$ en considérant que si sa valeur est inférieure à 1.96, on considérera la t.value comme non significative.

```{r}
ordres <- expand.grid(ar = 0:4, ma = 0:6)[-1, ]
modeles <- list()

for (i in seq(nrow(ordres))) {
  modeles[[i]] <- Arima(Zt, order = c(ordres[i, "ar"], 0, ordres[i, "ma"]))
}

resultats <- data.frame(AR = rep(NA, nrow(ordres)),
                      MA = rep(NA, nrow(ordres)),
                      AIC = rep(NA, nrow(ordres)),
                      stringsAsFactors = FALSE)
for (i in seq(nrow(ordres))) {
  model <- modeles[[i]]
  coefs <- model$coef
  se_coefs <- sqrt(diag(vcov(model)))
  t.values <- abs(coefs / se_coefs)
  compteur_t.values <- sum(t.values < 1.96) - 1

  resultats[i, "AR"] <- ordres[i, "ar"]
  resultats[i, "MA"] <- ordres[i, "ma"]
  resultats[i, "AIC"] <- AIC(model)
  resultats[i, "t.value non significatives"] <- compteur_t.values
}

kable(resultats, booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

Il y a deux modèles qui se distinguent clairement des autres : ARMA(1,3) et l'ARMA(4,1). Ils ont toutes leurs t.values significatives et ont un AIC plus faible que les autres. On gardera l'ARMA(1,3) car son AIC est un peu plus faible que celui de l'ARMA(4,1), bien qu'on puisse considérer les deux modèles comme équivalents du point de vue de ce critère.

## e) Prévision pour le mois de janvier

Pour la fonction forecast, on choisit un h = 4, correspondant aux 4 premières semaines de janvier.

```{r}
arma_xt <- Arima(Zt, order=c(1,0,3))
Zt_forecast <- forecast(arma_xt, h=4)
```

On doit ensuite reconstruire les prédictions pour $X_t$ en ajoutant la trend et la saisonnalité.

```{r}
t_future <- length(p1) + 1:length(Zt_forecast$mean)
sin_term_future <- sin(2 * pi * t_future / 52)
cos_term_future <- cos(2 * pi * t_future / 52)
Xt_future <- m + A * sin_term_future + B * cos_term_future + Zt_forecast$mean
Xt_future
```

```{r}
time_future <- seq(max(time(p1)), by = 1/52, length.out = 4)
plot(p1, main = "Données originales avec prédictions", xlab = "Temps", ylab = "Valeurs", type = "l")
points(time_future, Xt_future, col = "blue", pch = 19)
lines(time_future, Xt_future, col = "red", lty = 2)
legend("topleft", legend = c("Originale", "Prédictions"), col = c("black", "blue"), pch = c(NA, 19), lty = c(1, NA))
```
Les prédictions semblent suivre la tendance, continuant la hausse en cours au mois de décembre.

## f) SARIMA

```{r}
sarima_Xt <- auto.arima(p1)
summary(sarima_Xt)
```

La fonction `auto.arima()` choisi le modèle $SARIMA_{52}[(0,0,3)(2,1,0)]$. Ainsi, il choisi 3 composantes MA pour la partie non saisonnière et 2 composantes AR ainsi qu'une différenciation pour la partie saisonnière.

```{r}
january_forecast <- forecast(sarima_Xt, h=4)
january_forecast
```


## g) Application de l'opérateur de différenciation $(1-B^{52})$

```{r}
p1_diff <- diff(p1, lag = 52)
eacf(p1_diff)
arma_model_diff <- auto.arima(p1_diff, seasonal = FALSE)
summary(arma_model_diff)
january_forecast_diff <- forecast(arma_model_diff, h=4)
kbl(january_forecast_diff, booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
AIC(arma_model_diff)
```
Avec une différenciation de 52 sur un modèle de type ARMA, on revient au même résultat que pour $Z_t$. La méthode du coin et la fonction auto.arima considère $ARMA(1,3)$ comme le meilleur modèle.

## h) Modèle le plus pertinent

Effectuer une comparaison d'AIC est pertinent pour choisir le meilleur modèle. 

```{r}
df <- data.frame(
  Modèle = character(0),
  AIC = numeric(0),
  MAE = numeric(0)
)

df <- rbind(df, data.frame(Modèle = "Modèle C", AIC = AIC(reg_modele), MAE = mean(abs(reg_modele$residuals))))
df <- rbind(df, data.frame(Modèle = "Modèle E", AIC = arma_xt$aic, MAE = mean(abs(arma_xt$residuals))))
df <- rbind(df, data.frame(Modèle = "Modèle F", AIC = sarima_Xt$aic, MAE = mean(abs(sarima_Xt$residuals))))
df <- rbind(df, data.frame(Modèle = "Modèle G", AIC = AIC(arma_model_diff), MAE = mean(abs(arma_model_diff$residuals))))

kable(df, booktabs = TRUE) %>%
  kable_styling(latex_options = c("HOLD_position"))
```

On choisit donc le modèle estimé en e, c'est celui qui a l'AIC le plus faible. De plus, c'est celui qui semble faire le moins d'erreurs de prédiction obtenant un $MAE = 0.4$. Cela signifie qu'en moyenne, les prédictions du modèle e s'écartent de 0.40€ du prix réel de la série p1.

## i)

Forme réduite : $(1 - B^{52})(1 - 2B + B^2)(1 - 2B)X_t = (1-B) \varepsilon_t$

```{r}
set.seed(123)
x <- sim_sarima(n = 500,
                model = list(ar = c(2, -2, 1),
                             ma = 1,
                             iorder = 1, 
                             siorder = 1,
                             nseasons = 52,
                             sigma2 = 1))


plot(ts(x))

# Test de périodicité et stationnarité de Xt
acf(ts(x))
pacf(ts(x))
adf.test(x)


# Série différenciée de Xt, Xt - Xt-1
y <- diff(x,1)

plot(ts(y))

# Test de périodicité et stationnarité de Xt - Xt-1
acf(ts(y))
pacf(ts(y))
adf.test(y)

# Série Yt
model_z <- Arima(ts(rnorm(500), frequency = 52), order = c(0, 1, 1), seasonal = list(order = c(0, 1, 1)))

z <- simulate(model_z, nsim = 500)

plot(ts(z))

acf(ts(z))
pacf(ts(z))
adf.test(z)
```
La série temporelle \( X_t \) n'est pas stationnaire, et cela reste vrai même après différenciation. Cependant, la série différenciée révèle une saisonnalité, comme on peut le voir dans le graphique de la fonction d'autocorrélation (ACF), où des cycles de significativité apparaissent à certains décalages (lags). En revanche, la série \( Y_t \) semble être stationnaire. Ceci est indiqué par le fait que l'autocorrélation de ses décalages, observée à la fois dans l'ACF et dans la fonction d'autocorrélation partielle (PACF), est très faible. De plus, le test de Dickey-Fuller vient confirmer cette hypothèse en rejetant $H0$ qui suppose que \( Y_t \) est non-stationnaire.

## j) k)

```{r}
eacf(z)
modele_ar <- Arima(z, order = c(0, 0, 0))
modele_ma <- Arima(z, order = c(0, 0, 0))
modele_arma <- Arima(z, order = c(0, 0, 0))

forecast_1 <- forecast(modele_arma, h = 1)
forecast_2 <- forecast(modele_arma, h = 2)

print(forecast_1)
print(forecast_2)
```

On choisit le modèle AR(0), MA(0) et donc ARMA(0,0). On en conclut que c'est un bruit blanc. Notre hypothèse est que 
Concernant la question j), nous avons déterminé que le modèle est AR(0) et MA(0), alors les formes $AR(\infty)$ et $MA(\infty)$ sont simplement le terme constant, car il n'y a pas de termes dépendant des valeurs passées ou des erreurs passées.

## l)

```{r}
diff_ordre <- 1
diff_order_saison <- 1
periode <- 52
Y_t <- diff(diff(x, differences = diff_ordre), 
            differences = diff_order_saison, 
            lag = periode)

model_Y <- Arima(Y_t, order = c(1, 0, 0))

phi <- model_Y$coef[1]

n_simulations <- 999
n <- 500

forecasts_501 <- numeric(n_simulations)
forecasts_502 <- numeric(n_simulations)

for (i in 1:n_simulations) {
    series <- arima.sim(model = list(ar = c(phi)), n = n)
    fit <- Arima(series, order = c(1,0,0))
    forecast <- forecast(fit, h = 2)
    
    forecasts_501[i] <- forecast$mean[1]
    forecasts_502[i] <- forecast$mean[2]
}

mean_501 <- mean(forecasts_501)
mean_502 <- mean(forecasts_502)
sd_501 <- sd(forecasts_501)
sd_502 <- sd(forecasts_502)

hist(forecasts_501, main = "Distribution des Prévisions pour Y_501", xlab = "Y_501")
hist(forecasts_502, main = "Distribution des Prévisions pour Y_502", xlab = "Y_502")

print(paste("Moyenne des prévisions pour Y_501:", mean_501, "et écart-type:", sd_501))
print(paste("Moyenne des prévisions pour Y_502:", mean_502, "et écart-type:", sd_502))
```

Les distributions des deux prévisions sont normales et centrées à zéro, notre modèle AR(1) prédit donc en moyenne une valeur proche de 0 pour les pas futurs. Les valeurs des écarts-types montrent que les prévisions se situent relativement proche de la moyenne. De plus, ces écarts types sont assez faibles, justifiant que les prédictions sont assez précises.