---
title: "Script"
output: pdf_document
date: "2024-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(tseries)
library(forecast)
library(kableExtra)
library(knitr)
```

# Exercice 1

## a) $X_t = \sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}$

**1. Espérance**

\begin{align*}
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)] + \mathbb{E}[\varepsilon_t] - 0.2\mathbb{E}[\varepsilon_{t-1}] \\
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)] + 0 - 0.2 \times 0 \\
\mathbb{E}[X_t] &= \mathbb{E}[\sin(t)]
\end{align*}

Par conséquent, l'espérance de $X_t$ n'est pas constante car elle dépend de t à travers le terme $\sin(t)$.

**2. Autocovariance**

\[ \gamma_X(h) = \text{Cov}(\sin(t) + \varepsilon_t - 0.2 \times \varepsilon_{t-1}, \sin(t+h) + \varepsilon_{t+h} - 0.2 \times \varepsilon_{t+h-1}) \]

La covariance des sinus s'annule donc :

\begin{align*}
\gamma_X(0)   &= \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) \\
\gamma_X(1)   &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t+1} - 0.2 \times \varepsilon_t) \\
\gamma_X(-1)  &= \text{Cov}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}, \varepsilon_{t-1} - 0.2 \times \varepsilon_{t-2})
\end{align*}


- \( h = 0 \) : \( \gamma_X(0) = \text{Var}(\varepsilon_t - 0.2 \times \varepsilon_{t-1}) = \sigma^2 + 0.04\sigma^2 = 1.04\sigma^2 \) (car \( \sigma^2 = 1 \) dans les simulations).
- \( h = 1 \) ou \( h = -1 \) : \( \gamma_X(1) = \gamma_X(-1) = -0.2\sigma^2 \).

Le processus n'est ainsi pas stationnaire puisque son espérance n'est pas constante.

**3. Décomposition saisonnière**

Pour rendre ce processus stationnaire, nous allons enlever la composante de saisonnalité. Ainsi :

\(Y_t = X_t - \sin(t) = \varepsilon_t - 0.2 \times \varepsilon_{t-1}\).

Le processus peut ainsi être réécris sous la forme $X_t = (1-0.2B)\epsilon_t$, correspondant à un MA(1).

```{r}
set.seed(123)
a1 <- arima.sim(model = list(ma = 0.2), n = 200)
a2 <- arima.sim(model = list(ma = 0.2), n = 200)
a3 <- arima.sim(model = list(ma = 0.2), n = 200)

plot(a1, type="l", col="black", main="Trajectoires MA(1)",
     xlab = "Temps", ylab = "Valeurs")
lines(a2, col="limegreen")
lines(a3, col="lightblue")
```

## b) $\, X_t = \varepsilon_t - \varepsilon_{t-1}$

**1. Espérance**

\[
E(X_t) = E(\varepsilon_t - \varepsilon_{t-1}) = E(\varepsilon_t) - E(\varepsilon_{t-1}) = 0 - 0 = 0
\]

**2. Autocovariance**

- Pour \( h = 0 \) :
\begin{align*}
\gamma_X(0) &= \text{Cov}(X_t, X_{t + 0}) \\
&= \text{Var}(X_t) \\
&= \text{Var}(\varepsilon_t) + \text{Var}(\varepsilon_{t-1}) \\
&= \sigma^2 + \sigma^2 \\
&= 2
\end{align*}

- Pour \( h = 1 \) :
\begin{align*}
\gamma_X(1) &= \text{Cov}(X_t, X_{t+1}) \\
&= \text{Cov}(\varepsilon_t - \varepsilon_{t-1}, \varepsilon_{t+1} - \varepsilon_t) \\
&= \text{Cov}(\varepsilon_t, \varepsilon_{t+1}) - \text{Cov}(\varepsilon_t, \varepsilon_t) - \text{Cov}(\varepsilon_{t-1}, \varepsilon_{t+1}) + \text{Cov}(\varepsilon_{t-1}, \varepsilon_t)
\end{align*}

D'après la définition du bruit blanc, $\text{Cov}(\varepsilon_t, \varepsilon_s) = 0, \quad t \neq s$, alors :
\begin{align*}
\gamma_X(1) &= 0 - \text{Var}(\varepsilon_t) - 0 + 0 \\
&= -\text{Var}(\varepsilon_t) \\
&= -1
\end{align*}

Le processus \( X_t = \varepsilon_t - \varepsilon_{t-1} \) est stationnaire au second ordre car son espérance est constante et sa fonction d'autocovariance ne dépend que de \( h \) et non de \( t \).


**3. Forme canonique**

Le processus, sous sa forme canonique, s'écrit comme : $X_t = (1 - B)\varepsilon_t$ correspondant à un un MA(1).

```{r}
set.seed(123)
b1 <- arima.sim(model = list(ma = -1), n = 200)
b2 <- arima.sim(model = list(ma = -1), n = 200)
b3 <- arima.sim(model = list(ma = -1), n = 200)

plot(b1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(b2, col="limegreen")
lines(b3, col="lightblue")
```

## c) $X_t = A\cos(\omega t) + B\sin(\omega t)$

**1. Espérance**

$E[X_t] = E[A]\cos(\omega t) + E[B]\sin(\omega t)$
$E[X_t] = E[A] \times E[\cos(\omega t)] + E[B] \times E[\sin(\omega t)] = 0$

**2. Autocovariance**
\begin{align*}
\gamma(s, t) &= E\left[(A\cos(\omega s) + B\sin(\omega s))(A\cos(\omega t) + B\sin(\omega t))\right] \\
&= E\left[A^2\cos(\omega s)\cos(\omega t) + AB\cos(\omega s)\sin(\omega t) + AB\sin(\omega s)\cos(\omega t) + B^2\sin(\omega s)\sin(\omega t)\right]
\end{align*}


Étant donné que $A$ et $B$ sont indépendantes, on peut simplifier cette expression en utilisant \( E[A^2] \), \( E[B^2] \) et le fait que \( E[AB] = E[A]E[B] = 0 \) :

\[ \gamma(s, t) = E[A^2]\cos(\omega s)\cos(\omega t) + E[B^2]\sin(\omega s)\sin(\omega t) \]

Puisque \( A \) et \( B \) ont une variance finie on peut les remplacer par leurs variances :

\[ \gamma(s, t) = \sigma_A^2\cos(\omega s)\cos(\omega t) + \sigma_B^2\sin(\omega s)\sin(\omega t) \]

\[
\gamma(s, t) = \sigma_A^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) + \cos(\omega s + \omega t)] + \sigma_B^2 \cdot \frac{1}{2}[\cos(\omega s - \omega t) - \cos(\omega s + \omega t)]
\]

\[
\gamma(s, t) = \frac{1}{2}(\sigma_A^2 + \sigma_B^2) \cos(\omega(t - s))
\]

Le processus \( X_t = A\cos(\omega t) + B\sin(\omega t) \) est stationnaire au second ordre, car son espérance est constante et sa fonction d'autocovariance dépend uniquement de la différence entre les temps \( s \) et \( t \).

```{r}
set.seed(123)
n <- 200
omega <- pi/30
A <- rnorm(n)
B <- rnorm(n)

t <- 1:n
trajectoires <- matrix(0, nrow = n, ncol = 3)

for (i in 1:3) {
  trajectoires[, i] <- A[i] * cos(omega * t) + B[i] * sin(omega * t)
}

plot(t, trajectoires[, 1], type = 'l', col = 'black', ylim = range(trajectoires),
     xlab = 'Time', ylab = 'X_t', main = 'Trajectoires')
lines(t, trajectoires[, 2], col = 'limegreen')
lines(t, trajectoires[, 3], col = 'lightblue')
```

## d) $X_t = 2X_{t-1} - X_{t-2} + \varepsilon_t$

**1. Espérance :**
\begin{align*}
\mathbb{E}[X_t] & = 2\mathbb{E}[X_t] - \mathbb{E}[X_t] + \mathbb{E}[\varepsilon_t] \\
\mathbb{E}[X_t] & = \mathbb{E}[X_t] + 0
\end{align*}

Le processus est égale à lui-même et donc constant dans le temps.

**3. Forme canonique**

Le modèle peut se réécrire sous sa forme canonique comme : $(1 - 2B + B^2) X_t = \varepsilon_t$, correspondant à un AR(2).

On cherche les racines du polynôme caractéristique $1-2r+r^2=0$ :

\[ r = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \]

Dans notre cas, \( a = 1 \), \( b = -2 \) et \( c = 1 \). Les racines seront donc :

\[ r = \frac{-(-2) \pm \sqrt{(-2)^2 - 4 \cdot 1 \cdot 1}}{2 \cdot 1} \]
\[ r = \frac{2 \pm \sqrt{4 - 4}}{2} \]
\[ r = \frac{2 \pm \sqrt{0}}{2} \]
\[ r = \frac{2}{2} \]
\[ r = 1 \]

Le processus n'est donc pas stationnaire puisque les racines sont sur le cercle unité.

**4. Différenciation**

Pour le rendre stationnaire, on va différencier le processus à l'ordre 2.

\[ \Delta X_t = (2X_{t-1} - X_{t-2} + \varepsilon_t) - X_{t-1} \]
\[ \Delta X_t = X_{t-1} - X_{t-2} + \varepsilon_t \]

\[ \Delta^2 X_t = \Delta X_t - \Delta X_{t-1} \]
\[ \Delta^2 X_t = (X_{t-1} - X_{t-2} + \varepsilon_t) - (X_{t-2} - X_{t-3} + \varepsilon_{t-1}) \]
\[ \Delta^2 X_t = X_{t-1} - 2X_{t-2} + X_{t-3} + \varepsilon_t - \varepsilon_{t-1} \]

Le processus correspond donc à un ARIMA(2,2,0).

```{r}
processus_d <- function(n) {
  epsilon <- rnorm(n)
  X <- rep(0, n)
  delta2_X <- rep(0, n)

  X[1] <- rnorm(1)
  X[2] <- rnorm(1)
  X[3] <- rnorm(1)

  for (t in 4:n) {
    X[t] <- 2*X[t-1] - X[t-2] + epsilon[t]
    delta2_X[t] <- X[t-1] - 2*X[t-2] + X[t-3] + epsilon[t] - epsilon[t-1]
  }

  return(delta2_X)
}

n = 200
d1 <- processus_d(n)
d2 <- processus_d(n)
d3 <- processus_d(n)

plot(d1, type="l", col="black", main="Trajectoires", xlab="Temps", ylab="Valeurs")
lines(d2, col="limegreen")
lines(d3, col="lightblue")
```


## e) $\quad X_t - 4X_{t-1} = \varepsilon_t - 0.25\varepsilon_{t-1}$

Nous pouvons réécrire le processus comme $(1 - 4B)X_t = (1 - 0.25B)\varepsilon_t$.

Solution de l'équation caractéristique pour la partie AR :

$1-4r=0$
$r=0.25$

Solution de l'équation caractéristique pour la partie MA :

$1-0.25r=0$
$r=4$

La racine est l’extérieur du cercle unité, on inverse et on trouve $r=0.25$. Notre processus est donc un ARMA(0.25, 0.25).


```{r}
set.seed(123)
e1 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)
e2 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)
e3 <- arima.sim(model = list(ma = 0.25, ar = 0.25), n = 200)

plot(e1, type="l", col="black", main="Trajectoires MA(1)", xlab = "Temps", ylab = "Valeurs")
lines(e2, col="limegreen")
lines(e3, col="lightblue")
```


# Exercice 2

## a) Représentation graphique de la série p1

```{r}
source("prix.R")
plot(p1, type = "l", main = "Série Temporelle des Prix", xlab = "Temps", ylab = "Prix")
```
D'après le graphique, on peut émettre l'hypothèse que la série est stationnaire en moyenne et en variance puisque la moyenne est constante et la variance homoscédastique. De plus, la covariance entre 2 points semble constante, l'écart entre chaque fluctuation est le même et laisse penser à une saisonnalité. On peut cependant constater une légère tendance à la baisse à partir de 2019.

## b) Fonctions d'autocorrélation acf et pacf

```{r}
acf(p1)
pacf(p1)
spectrum(p1)
p1_decomp <- decompose(p1)
plot(p1_decomp)
```
On remarque que la saisonalité est très marquée et régulière, tandis que la tendance est plus compliquée à discerner, même s'il semble y avoir une tendance à la baisse.

## c) $X_t = m + A \sin\left(\frac{2\pi t}{T}\right) + B \cos\left(\frac{2\pi t}{T}\right) + Y_t$

   - \( X_t \) représente la valeur de la série temporelle au temps \( t \).
   - \( m \) est la moyenne de la série.
   - \( A \) et \( B \) sont les amplitudes des composantes sinusoïdales et cosinusoïdales, respectivement.
   - \( T \) est la période de la composante saisonnière. Pour des données hebdomadaires sur plusieurs années, \( T \) pourrait être 52, représentant le nombre de semaines par an.
   - \( Y_t \) est le terme d'erreur ou la composante non-périodique.

On choisit donc un T de 52.

```{r}
T <- 52
t <- 1:length(p1)

sin_term <- sin(2 * pi * t / T)
cos_term <- cos(2 * pi * t / T)

reg_model <- lm(p1 ~ sin_term + cos_term)

m <- coef(reg_model)[1]
A <- coef(reg_model)[2]
B <- coef(reg_model)[3]

Zt <- reg_model$residuals - mean(reg_model$residuals)

adf_test <- adf.test(Zt)
summary(reg_model)
adf_test
```

On remarque que tous les coefficients sont significatifs. La composante saisonnière sinusoïdale est donc une caractéristique importante de la série p1. L'écart type résiduel est faible (0.5328) et le coefficient de détermination ajusté est plutôt élevé (0.862), le modèle semble donc expliqué une grande partie de la variance des données.

Le p.value du test de Dickey-Fuller étant inférieur au seuil de 5%, on rejette l'hypothèse nulle d'une racine unitaire. Les résidus $Z_t$ du modèle sont stationnaires.

## d) Choix du modèle

```{r}
acf(Zt)
pacf(Zt)
```

$Z_t$ n'est pas un bruit blanc puisqu'il y a des autocorrelations significatives sur l'acf comme sur le pacf, suggérant que notre modèle contiendra une partie AR et une partie MA. Sur la partie AR, le dernier retard est atteint en lag = 4 et pour la partie MA, le dernier retard est atteint en lag = 6. Nous allons ainsi tester par le moyen d'une boucle tous les modèles en faisant varier la partie AR de 0 à 4 et la partie MA de 0 à 6. Pour chaque modèle, on vérifiera les t.values associées à chaque coefficients $coef/s.e.$ en considérant que si sa valeur est inférieure à 1.96, considère la t.value comme non significative.

```{r}
orders <- expand.grid(ar = 0:4, ma = 0:6)[-1, ]
models <- list()

for (i in seq(nrow(orders))) {
  models[[i]] <- Arima(Zt, order = c(orders[i, "ar"], 0, orders[i, "ma"]))
}

results <- data.frame(AR = rep(NA, nrow(orders)),
                      MA = rep(NA, nrow(orders)),
                      AIC = rep(NA, nrow(orders)),
                      stringsAsFactors = FALSE)
for (i in seq(nrow(orders))) {
  model <- models[[i]]
  coefs <- model$coef
  se_coefs <- sqrt(diag(vcov(model)))
  t.values <- abs(coefs / se_coefs)
  compteur_t.values <- sum(t.values < 1.96) - 1

  results[i, "AR"] <- orders[i, "ar"]
  results[i, "MA"] <- orders[i, "ma"]
  results[i, "AIC"] <- AIC(model)
  results[i, "t.value non significatives"] <- compteur_t.values
}

kable(results, booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```

Il y a deux modèles qui se distinguent clairement des autres : ARMA(1,3) et l'ARMA(4,1). Ils ont toutes leurs t.values significatives et ont un AIC plus faible que les autres. On gardera l'ARMA(1,3) car son AIC est un petit peu plus faible que pour l'ARMA(4,1), bien qu'on puisse considérer les deux modèles comme équivalents.

## e) Prévision pour le mois de janvier

Pour la fonction forecast, on choisit un h = 4, correspondant aux 4 premières semaines de janvier.

```{r}
arma_xt <- Arima(Zt, order=c(1,0,3))
Zt_forecast <- forecast(arma_xt, h=4)
```

On doit ensuite reconstruire les prédictions pour $X_t$ en ajoutant la trend et la saisonalité.

```{r}
t_future <- length(p1) + 1:length(Zt_forecast$mean)
sin_term_future <- sin(2 * pi * t_future / 52)
cos_term_future <- cos(2 * pi * t_future / 52)
Xt_future <- m + A * sin_term_future + B * cos_term_future + Zt_forecast$mean
Xt_future
```

## f) SARIMA

```{r}
sarima_Xt <- auto.arima(p1)
summary(sarima_Xt)
```

La fonction `auto.arima()` choisi le modèle $SARIMA_{52}[(0,0,3)(2,1,0)]$. Ainsi, il choisi 3 composantes MA pour la partie non saisonnière et 2 composantes AR ainsi qu'une différenciation pour la partie saisonnière.

```{r}
january_forecast <- forecast(sarima_Xt, h=4)
january_forecast
```

## g) Application de l'opérateur de différenciation $(1-B^52)$

```{r}
p1_diff <- diff(p1, lag = 52)
arma_model_diff <- auto.arima(p1_diff, seasonal = FALSE)
summary(arma_model_diff)
january_forecast_diff <- forecast(arma_model_diff, h=4)
kbl(january_forecast_diff, booktabs = TRUE) %>%
  kable_styling(latex_options = c("hold_position"))
```

## h) Modèle le plus pertinent